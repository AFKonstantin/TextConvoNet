{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import os\n",
    "'''\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca03903",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import json\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "'''\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# In[3]:\n",
    "trainfile = bz2.BZ2File('../input/amazonreviews/train.ft.txt.bz2','r')\n",
    "lines = trainfile.readlines()\n",
    "\n",
    "sent_analysis = []\n",
    "def sent_list(docs,splitStr='__label__'):\n",
    "    for i in range(1,len(docs)):\n",
    "        text=str(lines[i])\n",
    "        splitText=text.split(splitStr)\n",
    "        #print(i)\n",
    "        secHalf=splitText[1]\n",
    "        text=secHalf[2:len(secHalf)-1]\n",
    "        sentiment=secHalf[0]\n",
    "        sent_analysis.append([text,sentiment])\n",
    "    return sent_analysis\n",
    "\n",
    "sentiment_list=sent_list(lines[:1000000],splitStr='__label__')\n",
    "\n",
    "train_df = pd.DataFrame(sentiment_list,columns=['Text','Sentiment'])\n",
    "\n",
    "data_train=train_df[:4000]\n",
    "data_test=train_df[4000:5000]\n",
    "'''\n",
    "#a=input('path of the taining dataset with fields as title and tag(0,1) ')\n",
    "#b=input('path of test dataset')\n",
    "#data_train=pd.read_csv('../input/kuc-hackathon-winter-2018/drugsComTrain_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c7756",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train=pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "data_test=pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "\n",
    "data_train=data_train[:40000]\n",
    "data_test=data_test[40000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.rename(columns={'review':'title','sentiment':'tag'},inplace=True)\n",
    "data_test.rename(columns={'review':'title','sentiment':'tag'},inplace=True)\n",
    "\n",
    "#data_train['rating'].value_counts()\n",
    "#print('training_dataset',data_train)\n",
    "#print('training_dataset',data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ac0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(data_train)\n",
    "\n",
    "def make_tags(x):   #converting the ratings column into 0's and 1's.  for binary classifier to take place\n",
    "    if(x==\"negative\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7e1cc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_train['tag']=data_train['tag'].apply(lambda x: make_tags(x))\n",
    "data_test['tag']=data_test['tag'].apply(lambda x: make_tags(x))\n",
    "\n",
    "#print(data_train)\n",
    "\n",
    "count0=(data_train['tag']==0).sum()\n",
    "count1=(data_train['tag']==1).sum()\n",
    "if(count0>count1):\n",
    "    imbalance_ratio=(count0)/count1\n",
    "else:\n",
    "    imbalance_ratio=(count1)/count0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d720f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "print('imbalance_ratio',imbalance_ratio)\n",
    "#print(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15be61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def no_of_words_in_paragraph(x):\n",
    "    return len(list(x))\n",
    "\n",
    "data_train['no_of_words_in_paragraph']=data_train['title'].apply(lambda x:no_of_words_in_paragraph(x))\n",
    "\n",
    "data_test['no_of_words_in_paragraph']=data_test['title'].apply(lambda x:no_of_words_in_paragraph(x))\n",
    "\n",
    "\n",
    "\n",
    "print(data_train)\n",
    "avg=data_train['no_of_words_in_paragraph'].mean()\n",
    "maxim=data_train['no_of_words_in_paragraph'].max()\n",
    "print('average paragraph length',data_train['no_of_words_in_paragraph'].mean())\n",
    "print('maximum para length',data_train['no_of_words_in_paragraph'].max())\n",
    "print('hii')\n",
    "excess=(data_train['no_of_words_in_paragraph']>avg).sum()\n",
    "excess_ratio=excess/len(data_train)\n",
    "print('excess_ratio',excess_ratio)\n",
    "\n",
    "\n",
    "#applying sentence tokenizer\n",
    "import nltk.data \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') \n",
    "# Loading PunktSentenceTokenizer using English pickle file \n",
    "def make_sent_token(x):\n",
    "    return tokenizer.tokenize(x) \n",
    "#converting each paragraph into separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ec93d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_train['sentence_token']=data_train['title'].apply(lambda x: make_sent_token(x))\n",
    "\n",
    "data_test['sentence_token']=data_test['title'].apply(lambda x: make_sent_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93722401",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_train.drop(columns=['uniqueID','date','usefulCount','condition','drugName'],inplace=True,axis=1)# dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f29f66",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_test.drop(columns=['uniqueID','date','usefulCount','condition','drugName'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4474b96",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7ec48",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_train['no_of_sentences']=data_train['sentence_token'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c49f28",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_test['no_of_sentences']=data_test['sentence_token'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5da01",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "avg_sen_length=data_train['no_of_words_in_paragraph'].sum()/data_train['no_of_sentences'].sum()\n",
    "print(avg_sen_length)\n",
    "\n",
    "#max(data_train['no_of_sentences'])##no of rows in sentence matrix which is to be feed in model(max number of sentence in any paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71a623",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#len(data_train[data_train['no_of_sentences']==92]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa39ac4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#max(data_test['no_of_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad99db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def max_length_of_sentence(x,y):\n",
    "    sen=x\n",
    "    nu=y\n",
    "    #print(sen)\n",
    "    ma=0\n",
    "    if(nu>1):\n",
    "        l=sen.split('.')\n",
    "        #print(l)\n",
    "        for i in range(len(l)):\n",
    "            k=l[i].replace(',','')\n",
    "            maxi=len(k.split())\n",
    "            #print(maxi)\n",
    "            if(maxi>ma):\n",
    "                ma=maxi\n",
    "        return ma\n",
    "    else:\n",
    "        return len(sen.split())\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa330a80",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_train['max_words_in_sentence']=data_train.apply(lambda x: max_length_of_sentence(x.title,x.no_of_sentences),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65e489",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_test['max_words_in_sentence']=data_test.apply(lambda x: max_length_of_sentence(x.title,x.no_of_sentences),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394759b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#max(data_train['max_words_in_sentence'])## number of columns in the data to be feeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a8d94",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "x1=max(data_train['no_of_sentences'])\n",
    "y1=max(data_train['max_words_in_sentence'])\n",
    "\n",
    "x2=max(data_test['no_of_sentences'])\n",
    "y2=max(data_test['max_words_in_sentence'])\n",
    "\n",
    "if(x1>=x2):\n",
    "    m=x1\n",
    "    print(m)\n",
    "    m=m\n",
    "else:\n",
    "    m=x2\n",
    "    m=m\n",
    "    \n",
    "if(y1>=y2):\n",
    "    n=y1\n",
    "else:\n",
    "    n=y2\n",
    "\n",
    "#So each para will be converted to a m*n matrix\n",
    "if(m<5):\n",
    "    m=6\n",
    "else:\n",
    "    m+=2\n",
    "print('x1,x2,y1,y2',x1,x2,y1,y2)\n",
    "\n",
    "print(\"m-->\",m,n)\n",
    "#So each para will be converted to a m*n matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Major part starts here ..... Now converting the paragraph into required matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e6ee7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "def make_tokens(text):     ##Converting into single tokens in order to create the vocabulary\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "data_train['tokens']=data_train['title'].apply(lambda x: make_tokens(x))\n",
    "data_test['tokens']=data_test['title'].apply(lambda x: make_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a247483",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_train['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#from gensim import models\n",
    "#word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "#word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../input/glove6b300dtxt/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308303e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "para_max=max(training_sentence_lengths)\n",
    "\n",
    "vocab=len(TRAINING_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b79c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#len(TRAINING_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad36c4a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), char_level=False)\n",
    "tokenizer.fit_on_texts(data_train['title'])       # we assigned values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ef624",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc390466",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print(train_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7df795",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_train.to_csv('medic_train.csv')\n",
    "#data_test.to_csv('medic_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450180a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_train_seq(x):\n",
    "    return tokenizer.texts_to_sequences(x)\n",
    "data_train['train_seq']=data_train['sentence_token'].apply(lambda x:make_train_seq(x) )\n",
    "data_test['train_seq']=data_test['sentence_token'].apply(lambda x:make_train_seq(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1646c31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#(data_train['train_seq'])   # here every para has been encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dcbca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def padding(x):    #now padding each sentence to a length of n...number of columns\n",
    "    MAX_SENTENCE_LENGTH=n  #(no of columns)\n",
    "    return pad_sequences(x,maxlen=MAX_SENTENCE_LENGTH,padding='post')\n",
    "\n",
    "data_train['padded']=data_train['train_seq'].apply(lambda x:padding(x))\n",
    "data_test['padded']=data_test['train_seq'].apply(lambda x:padding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa3490",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#(data_train.padded[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd3f42",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "## More code adapted from the keras reference (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# prepare embedding matrix \n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\n",
    "EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n",
    "print(EMBEDDING_DIM)\n",
    "#num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    " #= np.zeros(len(train_word_index) + 1, EMBEDDING_DIM)\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
    " EMBEDDING_DIM))\n",
    "for word, i in train_word_index.items():\n",
    "    #print(\"sd\")\n",
    "    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n",
    "    if embedding_vector is not None:\n",
    "        train_embedding_weights[i] = embedding_vector\n",
    "print(train_embedding_weights.shape)\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        \n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "#embedding_layer = Embedding(num_words,\n",
    "                          #  EMBEDDING_DIM,\n",
    "                          #  embeddings_initializer=Constant(embedding_matrix),\n",
    "                          #  input_length=MAX_SEQUENCE_LENGTH,\n",
    "                          #  trainable=False)\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM=300\n",
    "#train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
    " #EMBEDDING_DIM))\n",
    "#for word,index in train_word_index.items():\n",
    " #train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "#print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28bde9b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_full_para(x):     #92 cross 192 matrix of a paragraph.   (m*n)\n",
    "    l=len(x)\n",
    "    h=m-l    #no. of extra rows to be added\n",
    "    z=[0]*h*n       #1D vector(#addding extra lines for zeroes as padding)\n",
    "    z=np.reshape(z,(h,n))    #reshaping it to match the dimension of paragraph\n",
    "    s=x.tolist()+z.tolist()\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2f215",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_train['full_para']=data_train['padded'].apply(lambda x : make_full_para(x))\n",
    "data_test['full_para']=data_test['padded'].apply(lambda x : make_full_para(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f232626",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_train.full_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86980a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_1d_para(x):\n",
    "    l=[]\n",
    "    for i in x:\n",
    "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
    "    return l\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b0c94",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "data_train['single_d_array']=data_train['full_para'].apply(lambda x: create_1d_para(x) )\n",
    "data_test['single_d_array']=data_test['full_para'].apply(lambda x: create_1d_para(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a72208",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#train_cnn_data=np.array(data_train['single_d_array'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5db67d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_cnn_data=np.array(data_train['single_d_array'].tolist())\n",
    "test_cnn_data=np.array(data_test['single_d_array'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_train=data_train['tag'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a309c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Startting the training')\n",
    "#from __future__ import print_function\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM,SpatialDropout1D,Reshape\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.layers import Conv2D, GlobalMaxPooling2D,MaxPool2D,MaxPool3D,GlobalAveragePooling2D,Conv3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcb2a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "filter_sizes = [1,2,3,4]\n",
    "num_filters = 32\n",
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "maxlen=m*n\n",
    "\n",
    "def get_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((m, n, 300))(x)\n",
    "    #print(x)\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2), \n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 3),\n",
    "                                                                                    activation='relu')(x)\n",
    "    \n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 4),\n",
    "                                                                                    activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 1), \n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 2), activation='relu')(x)\n",
    "    \n",
    "    conv_6 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 3),\n",
    "                                                                                    activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    maxpool_0 = MaxPool2D()(conv_0)\n",
    "    maxpool_0=Flatten()(maxpool_0)\n",
    "    maxpool_1 = MaxPool2D()(conv_1)\n",
    "    maxpool_1=Flatten()(maxpool_1)\n",
    "    maxpool_2 = MaxPool2D()(conv_2)\n",
    "    maxpool_2 = Flatten()(maxpool_2)\n",
    "    \n",
    "    maxpool_4 = MaxPool2D()(conv_4)\n",
    "    maxpool_4=Flatten()(maxpool_4)\n",
    "    maxpool_5 = MaxPool2D()(conv_5)\n",
    "    maxpool_5=Flatten()(maxpool_5)\n",
    "    maxpool_6 = MaxPool2D()(conv_6)\n",
    "    maxpool_6=Flatten()(maxpool_6)\n",
    "    #maxpool_7 = MaxPool2D()(conv_7)\n",
    "   # maxpool_7=Flatten()(maxpool_7)\n",
    "    z = concatenate([maxpool_0, maxpool_1,maxpool_2],axis=1)\n",
    "    w=concatenate([maxpool_4, maxpool_5,maxpool_6],axis=1)    \n",
    "    #w=concatenate([maxpool_4, maxpool_5,maxpool_6],axis=1)    \n",
    "    #z = concatenate([maxpool_0, maxpool_1,maxpool_2,maxpool_4, maxpool_5,maxpool_6],axis=1)\n",
    "    #z = concatenate([maxpool_0, maxpool_1,maxpool_4, maxpool_5],axis=1)\n",
    "    \n",
    "    #z = Flatten()(z)\n",
    "    z=concatenate([w,z],axis=1)\n",
    "    z=Dense(units=64,activation=\"relu\")(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "        \n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153775a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50065adc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "import time, datetime\n",
    "start = datetime.datetime.now()\n",
    "history=model.fit(train_cnn_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
    "end = datetime.datetime.now()\n",
    "diff1= (end - start)\n",
    "print('time taken by text_6',diff1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred=model.predict(test_cnn_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i][0]<0.5):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351ffbf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "def check_metric(output_class_pred,original_ans,diff1):\n",
    "    rightly_predicted=0\n",
    "    TP=0\n",
    "    for i in range(len(y_test)):\n",
    "        if(original_ans[i]==output_class_pred[i]):\n",
    "            rightly_predicted+=1\n",
    "        \n",
    "        \n",
    "    print(\"Overall_acuracy:\",rightly_predicted/len(output_class_pred))\n",
    "    print('TP',TP)\n",
    "    accuracy=rightly_predicted/len(y_test)\n",
    "    print(classification_report(original_ans,output_class_pred))\n",
    "    print(confusion_matrix(original_ans,output_class_pred))\n",
    "    TN=confusion_matrix(original_ans,output_class_pred)[0][0]\n",
    "    TP=confusion_matrix(original_ans,output_class_pred)[1][1]\n",
    "    FP=confusion_matrix(original_ans,output_class_pred)[0][1]\n",
    "    FN=confusion_matrix(original_ans,output_class_pred)[1][0]\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recalll=TP/(FN+TP)\n",
    "    F1=2*precision*recalll/(precision+recalll)\n",
    "    sensiti=TP/(TP+FN)\n",
    "    specifici=TN/(TN+FP)\n",
    "    numerator=TP*TN - FP*FN\n",
    "    \n",
    "    denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
    "    MCc=numerator/denominator\n",
    "    G_mean1=np.sqrt(sensiti*precision)\n",
    "    G_mean2=np.sqrt(sensiti*specifici)\n",
    "    print('precision:' ,TP/(TP+FP))\n",
    "    print('recall:',TP/(FN+TP))\n",
    "    print(\"F1:\",F1)\n",
    "    print(\"Specificity:\",TN/(TN+FP))\n",
    "    print(\"Sensitivity \",TP/(TP+FN))\n",
    "    print('G-mean1:',np.sqrt(sensiti*precision))\n",
    "    print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
    "    print(\"MCC :\",MCc)\n",
    "    acc=[]\n",
    "    pre=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    specificity=[]\n",
    "    sensitivity=[]\n",
    "    GMean1=[]\n",
    "    Gmean2=[]\n",
    "    MCC=[]\n",
    "    tp=[]\n",
    "    fp=[]\n",
    "    fn=[]\n",
    "    tn=[]\n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    recall.append(recalll)\n",
    "    f1.append(F1)\n",
    "    specificity.append(specifici)\n",
    "    sensitivity.append(sensiti)\n",
    "    GMean1.append(G_mean1)\n",
    "    Gmean2.append(G_mean2)\n",
    "    MCC.append(MCc)\n",
    "    tp.append(TP)\n",
    "    fp.append(FP)\n",
    "    tn.append(TN)\n",
    "    fn.append(FN)\n",
    "    data={'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn,\"traintime\":diff1,\"Exceeding_ratio\":excess_ratio,\"imbalance_ratio\":imbalance_ratio,\"Average_length_of_paragraph\":avg,\"Maximum_length_of_a_paragraph\":maxim,\"Average_length_of_sentences\":avg_sen_length,\"Maximum_length_of_a_sentence_in_a_paragraph\":n,\"Maximum_no_of_sentence_in_any_paragraph\":m,\"Vocabular_size\":vocab,\"label0\":count0,\"label1\":count1}\n",
    "    metric=pd.DataFrame(data)\n",
    "    return metric\n",
    "\n",
    "print(history.history.keys())\n",
    "    \n",
    "resi=check_metric(output_class_pred,original_ans,diff1)\n",
    "resi.to_csv('results_text.csv', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "filter_sizes = [1,2,3,4]\n",
    "num_filters = 32\n",
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "maxlen=m*n\n",
    "def get_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((m, n, 300))(x)\n",
    "    #print(x)\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2), \n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 3),\n",
    "                                                                                    activation='relu')(x)\n",
    "    \n",
    "    #conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 4),\n",
    "                                                                                    #activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 1), \n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 2), activation='relu')(x)\n",
    "    \n",
    "    #conv_6 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 3),\n",
    "                                                                                    #activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    maxpool_0 = MaxPool2D()(conv_0)\n",
    "    maxpool_0=Flatten()(maxpool_0)\n",
    "    maxpool_1 = MaxPool2D()(conv_1)\n",
    "    maxpool_1=Flatten()(maxpool_1)\n",
    "    #maxpool_2 = MaxPool2D()(conv_2)\n",
    "    #maxpool_2 = Flatten()(maxpool_2)\n",
    "    \n",
    "    maxpool_4 = MaxPool2D()(conv_4)\n",
    "    maxpool_4=Flatten()(maxpool_4)\n",
    "    maxpool_5 = MaxPool2D()(conv_5)\n",
    "    maxpool_5=Flatten()(maxpool_5)\n",
    "    #maxpool_6 = MaxPool2D()(conv_6)\n",
    "    #maxpool_6=Flatten()(maxpool_6)\n",
    "    #maxpool_7 = MaxPool2D()(conv_7)\n",
    "   # maxpool_7=Flatten()(maxpool_7)\n",
    "        \n",
    "    #w=concatenate([maxpool_4, maxpool_5,maxpool_6],axis=1)    \n",
    "    #z = concatenate([maxpool_0, maxpool_1,maxpool_2,maxpool_4, maxpool_5,maxpool_6],axis=1)\n",
    "    #z = concatenate([maxpool_0, maxpool_1,maxpool_4, maxpool_5],axis=1)\n",
    "    w=concatenate([maxpool_4, maxpool_5],axis=1)    \n",
    "    #z = concatenate([maxpool_0, maxpool_1,maxpool_2,maxpool_4, maxpool_5,maxpool_6],axis=1)\n",
    "    z = concatenate([maxpool_0, maxpool_1],axis=1)\n",
    "    \n",
    "    #z = Flatten()(z)\n",
    "    z=concatenate([w,z],axis=1)\n",
    "    #z = Flatten()(z)\n",
    "    #z=concatenate([w,z],axis=1)\n",
    "    z=Dense(units=64,activation=\"relu\")(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "        \n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403ff85",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f37a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8492f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "import time, datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "history=model.fit(train_cnn_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "diff1= (end - start)\n",
    "print('time taken by text_4',diff1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred=model.predict(test_cnn_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i][0]<0.5):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "\n",
    "\n",
    "resi=check_metric(output_class_pred,original_ans,diff1)\n",
    "resi.to_csv('results_text.csv', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fd2f7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## now perparing training data for yoon kim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_single_line_para(x):\n",
    "    l=[]\n",
    "    for i in x:\n",
    "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
    "    return l\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d774f41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_train['create_single_line_para']=data_train['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
    "data_test['create_single_line_para']=data_test['train_seq'].apply(lambda x: create_single_line_para(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "yoon_kim_train_data=np.array(data_train['create_single_line_para'].tolist())\n",
    "yoon_kim_train_data=pad_sequences(yoon_kim_train_data,maxlen=para_max,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf81f7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "yoon_kim_test_data=np.array(data_test['create_single_line_para'].tolist())\n",
    "yoon_kim_test_data=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
    "\n",
    "\n",
    "#from __future__ import print_function\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38570018",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_y=pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64570096",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trains_y=train_y[[0,1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b66160",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "maxlen=para_max \n",
    "max_sequence_length=para_max\n",
    "MAX_SEQUENCE_LENGTH=para_max\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "\n",
    "#model3 yoon kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b75095",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=2)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    #pool = MaxPooling1D(pool_size=2)(conv)\n",
    "\n",
    "    #if extra_conv==True:\n",
    "        #x = Dropout(0.01)(l_merge)  \n",
    "    #else:\n",
    "        # Original Yoon Kim model\n",
    "        #x = Dropout(0.001)(pool)\n",
    "    x = Flatten()(l_merge)\n",
    "    \n",
    "    x = Dropout(0.5)(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06275644",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model1 = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                 True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19253d39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data=yoon_kim_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b55bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "testing_data=yoon_kim_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafafc3b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "import time, datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "hist = model1.fit(training_data, trains_y,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
    "end = datetime.datetime.now()\n",
    "diff1= (end - start)\n",
    "print('time taken by yoon',diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec2279",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred=model1.predict(testing_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "#output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    m=max(y_test[i])\n",
    "    if(y_test[i].index(m)==0):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "resi=check_metric(output_class_pred,original_ans,diff1)\n",
    "\n",
    "resi.to_csv('results_text', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Embedding, concatenate\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, ZeroPadding1D\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate, Dot, Multiply, RepeatVector\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Lambda, Permute\n",
    "\n",
    "#from tensorflow.keras.layers.core import Reshape, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\n",
    "#from tensorflow.keras.constraints import maxnorm\n",
    "#from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def ConvNet_vdcnn(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    \n",
    "    \n",
    "# 4 pairs of convolution blocks followed by pooling\n",
    "    conv = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(embedded_sequences)\n",
    "    \n",
    "    \n",
    "# 4 pairs of convolution blocks followed by pooling\n",
    "    for filter_size in [64, 128, 256, 512]:\n",
    "    \n",
    "    # each iteration is a convolution block\n",
    "        for cb_i in [0,1]:\n",
    "            conv=(Conv1D(filter_size, 3, padding=\"same\",activation='relu'))(conv)\n",
    "        #model_1.add(BatchNormalization())\n",
    "        #model_1.add(Activation(\"relu\"))\n",
    "            conv=(Conv1D(filter_size, 1, padding=\"same\",activation='relu'))(conv)\n",
    "        #model_1.add(BatchNormalization())\n",
    "        #model_1.add(Activation(\"relu\"))\n",
    "    \n",
    "        conv=(MaxPooling1D(pool_size=2, strides=3))(conv)\n",
    "\n",
    "# model.add(KMaxPooling(k=2))\n",
    "    conv=(Flatten())(conv)\n",
    "    conv=(Dense(4096, activation=\"relu\"))(conv)\n",
    "    conv=(Dense(2048, activation=\"relu\"))(conv)\n",
    "    conv=(Dense(2048, activation=\"relu\"))(conv)\n",
    "#(Dense(9, activation=\"softmax\"))\n",
    "\n",
    "    preds = Dense(2, activation='softmax')(conv)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam',metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "model1 = ConvNet_vdcnn(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                 True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ae220",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data=yoon_kim_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef0f77",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "testing_data=yoon_kim_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd4f04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "import time, datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "hist = model1.fit(training_data, trains_y,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
    "end = datetime.datetime.now()\n",
    "diff1= (end - start)\n",
    "print('time taken by yoon',diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e8d02",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred=model1.predict(testing_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "#output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    m=max(y_test[i])\n",
    "    if(y_test[i].index(m)==0):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "resi=check_metric(output_class_pred,original_ans,diff1)\n",
    "\n",
    "resi.to_csv('results_text', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ConvNet_clstm(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [10, 20, 30, 40]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=64, kernel_size=filter_size, padding='valid', activation='relu')(embedded_sequences)\n",
    "        convs.append(l_conv)\n",
    "\n",
    "    cnn_feature_maps = Concatenate(axis=1)(convs)\n",
    "    sentence_encoder = LSTM(64,return_sequences=False)(cnn_feature_maps)\n",
    "    fc_layer =Dense(128, activation=\"relu\")(sentence_encoder)\n",
    "    #output_layer = Dense(9,activation=\"softmax\")(fc_layer)\n",
    "\n",
    "    #model_1 = Model(inputs=[text_input_layer], outputs=[output_layer])\n",
    "    preds = Dense(2, activation='softmax')(fc_layer)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model1 = ConvNet_clstm(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                 True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f513b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data=yoon_kim_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b7a12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "testing_data=yoon_kim_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b95e1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "import time, datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "hist = model1.fit(training_data, trains_y,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
    "end = datetime.datetime.now()\n",
    "diff1= (end - start)\n",
    "print('time taken by yoon',diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcec8ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred=model1.predict(testing_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "#output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    m=max(y_test[i])\n",
    "    if(y_test[i].index(m)==0):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ed563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "resi=check_metric(output_class_pred,original_ans,diff1)\n",
    "\n",
    "resi.to_csv('results_text', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ConvNet_lstm(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
    "    \n",
    "    \n",
    "    embedding_layer = Embedding(num_words,embedding_dim,weights=[embeddings],input_length=max_sequence_length,trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    sentence_encoder = LSTM(64,return_sequences=False)(embedded_sequences)\n",
    "    fc_layer =Dense(128, activation=\"relu\")(sentence_encoder)\n",
    "    #output_layer = Dense(9,activation=\"softmax\")(fc_layer)\n",
    "    #model_1 = Model(inputs=[text_input_layer], outputs=[output_layer])\n",
    "    preds = Dense(2, activation='softmax')(fc_layer)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "    \n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "model1 = ConvNet_lstm(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                 True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575ab08",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data=yoon_kim_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5584",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "testing_data=yoon_kim_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6b3dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "import time, datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "hist = model1.fit(training_data, trains_y,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
    "end = datetime.datetime.now()\n",
    "diff1= (end - start)\n",
    "print('time taken by yoon',diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c347b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred=model1.predict(testing_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "#output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    m=max(y_test[i])\n",
    "    if(y_test[i].index(m)==0):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed00fd6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "resi=check_metric(output_class_pred,original_ans,diff1)\n",
    "\n",
    "resi.to_csv('results_text', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
    "\n",
    "\n",
    "\n",
    "#resi.to_csv('results.csv', mode='a', index = False, header=resi.columns,columns=resi.columns)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
